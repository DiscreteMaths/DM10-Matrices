
%---------------------------------------------- %

% \[P(A |B = \frac{P(A \cap B)}{P(B)})\]

The complement rule in Probability

$P(C^{\prime}) = 1- P(C)$

 

If the probability of C is $70 \%$ then the probability of $C^{\prime}$ is $30\%$
\section{Matrices}

What are the dimensions of the following matrix


\[ \left(
\begin{array}{cc}
a_1 & a_2 \\ 
b_1 & b_2
\end{array} \right)\left(
\begin{array}{cc}
c_1 & d_1 \\ 
c_2 & d_2
\end{array} \right) = \left(
\begin{array}{cc}
(a_1 \times c_1) + (a_2 \times c_2) & (a_1 \times d_1) + (a_2 \times d_2) \\ 
(b_1 \times c_1) + (b_2 \times c_2) & (b_1 \times d_1) + (b_2 \times d_2)
\end{array} \right) \]

\bigskip
\large{
\[ \left(
\begin{array}{cc}
1 & 3 \\ 
0 & 2
\end{array} \right)\left(
\begin{array}{cc}
1 & 2 \\ 
4 & 1
\end{array} \right) = \left(
\begin{array}{cc}
(1 \times 1) + (3 \times 4) & (1 \times 2) + (3 \times 1) \\ 
(0 \times 4) + (2 \times 4) & (0 \times 2) + (2 \times 1)
\end{array} \right) = \left(
\begin{array}{cc}
14 & 5 \\ 
8 & 2
\end{array} \right) \]
}

\[ \left(
\left(
\begin{array}{cc}
1 & 2 \\ 
4 & 1
\end{array} \right)
\begin{array}{cc}
1 & 3 \\ 
0 & 2
\end{array} \right) = ? \]





Linear Algebra/Matrices
< Linear Algebra
This page may need to be reviewed for quality.

Jump to navigation
Jump to search

Contents
1
Matrices and Linear Transformations
2
Algebra of Transformations
2.1
Addition
2.2
Scalar multiplication
2.3
Multiplication
3
Corresponding algebra of matrices
3.1
Addition
3.2
Scalar multiplication
3.3
Matrix multiplication
3.3.1
Determinants of Products of Matrices (Binet's Theorem)
4
Matrices and system of linear equations
5
Exercises
6
See also
Matrices and Linear Transformations[edit]
It turns out that linear transformations can be represented in a 1-1 fashion in matrices. This chapter will be most likely be a review as the topic has already probably been covered in high school (see this link). The establishment of a one-to-one correspondence between linear transformations and matrices is very important in the study of linear transformations. 
Suppose you have a set of basis vectors x1, x2, x3, ..., xm of a vector space X and basis vectors y1, y2, y3, ..., yn of a vector space Y. 
Consider a linear transformation T from X to Y, and the vectors 
T(x1)=y1a11+y2a21+y3a31+...+ynan1,
T(x2)=y1a12+y2a22+y3a32+...+ynan2,
T(x3)=y1a13+y2a23+y3a33+...+ynan3,
...
T(xm)=y1a1m+y2a2m+y3a3m+...+ynanm,
You can arrange these coefficients in a matrix 
M = ( a 11 a 12 a 13 … a 1 m a 21 a 22 a 23 … a 2 m a 31 a 32 a 33 … a 3 m ⋮ ⋮ ⋮ ⋮ ⋮ a n 1 a n 2 a n 3 … a n m ) {\displaystyle M={\begin{pmatrix}a_{11}&a_{12}&a_{13}&\ldots &a_{1m}\\a_{21}&a_{22}&a_{23}&\ldots &a_{2m}\\a_{31}&a_{32}&a_{33}&\ldots &a_{3m}\\\vdots &\vdots &\vdots &\vdots &\vdots \\a_{n1}&a_{n2}&a_{n3}&\ldots &a_{nm}\\\end{pmatrix}}} 
. 
Thus, if you have any vector 
x = ∑ j = 1 m x j b j {\displaystyle x=\sum _{j=1}^{m}x_{j}b_{j}} 
, 
Then 
T ( x ) = T ( ∑ j = 1 m x j b j ) = ∑ j = 1 m b j T ( x j ) = ∑ j = 1 m b j ∑ i = 1 n y i a i j = ∑ i = 1 n y i ∑ j = 1 m a i j b j {\displaystyle T(x)=T(\sum _{j=1}^{m}x_{j}b_{j})=\sum _{j=1}^{m}b_{j}T(x_{j})=\sum _{j=1}^{m}b_{j}\sum _{i=1}^{n}y_{i}a_{ij}=\sum _{i=1}^{n}y_{i}\sum _{j=1}^{m}a_{ij}b_{j}} 
 
Thus, T(x) is a linear combination of basis vectors 
∑ i = 1 n y n c i {\displaystyle \sum _{i=1}^{n}y_{n}c_{i}} 
, where 
c i = ∑ j = 1 m a i j b j {\displaystyle c_{i}=\sum _{j=1}^{m}a_{ij}b_{j}} 
. 
Thus knowledge of a matrix in respect to bases can determine the value of a the result of a linear transformation. 
Thus, given any matrix, there is a corresponding function with the results being 
∑ i = 1 n y n c i {\displaystyle \sum _{i=1}^{n}y_{n}c_{i}} 
, where 
c i = ∑ j = 1 m a i j b j {\displaystyle c_{i}=\sum _{j=1}^{m}a_{ij}b_{j}} 
. 
This is obviously a linear operator, whose matrix coincides with the matrix used. This establishes the fact that every n by m matrix can determine a linear operator mapping an m dimensional vector space into an n dimensional vector space. 
Algebra of Transformations[edit]
Addition[edit]
Define the sum C=A+B where A and B are linear transformations to be the function C(x)=A(x)+B(x). One can easily verify that this is also a linear transformation. You can verify that given two linear transformations A and B, that 
A+B=B+A
(A+B)+C=C+(B+A)
A+0=A
A+(-A)=0
where 0 is the zero operator -A is the function -A(x) which one can easily verify to be a linear transformation. 
Scalar multiplication[edit]
Given a linear transformation a, define the function 
μ A {\displaystyle \mu A} 
 where 
μ {\displaystyle \mu } 
 is an element of a field to be the function 
( μ L ) ( x ) = μ ( L ( x ) ) {\displaystyle (\mu L)(x)=\mu (L(x))} 
. 
You can easily verify that given a linear transformations A and B and an elements of a field 
μ {\displaystyle \mu } 
, 
μ 1 {\displaystyle \mu _{1}} 
, and 
μ 2 {\displaystyle \mu _{2}} 
, that 
μ 1 ( μ 2 A ) = ( μ 1 μ 2 ) A {\displaystyle \mu _{1}(\mu _{2}A)=(\mu _{1}\mu _{2})A} 

1 A = A {\displaystyle 1A=A} 

( μ 1 + μ 2 ) A = μ 1 A + μ 2 A {\displaystyle (\mu _{1}+\mu _{2})A=\mu _{1}A+\mu _{2}A} 

μ ( A + B ) = μ A + μ B {\displaystyle \mu (A+B)=\mu A+\mu B} 

This implies that linear transformations form a vector space. 
